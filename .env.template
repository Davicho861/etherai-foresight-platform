# Template de variables de entorno para despliegue
# No compruebes este archivo en un entorno público con secretos reales.

# Tokens de acceso (exportar como variables de entorno en tu shell antes de ejecutar los scripts)
# RAILWAY_TOKEN: token para la CLI/API de Railway
# VERCEL_TOKEN: token para la CLI/API de Vercel
# OPENAI_API_KEY: clave de OpenAI (si usas integraciones que la requieran)

RAILWAY_TOKEN=
VERCEL_TOKEN=
OPENAI_API_KEY=

# Opcional: configuración para usar un LLM local vía Ollama
# Si deseas ejecutar el "cerebro" localmente instala Ollama y descarga un modelo:
#   ollama pull llama3
# Luego puedes apuntar al endpoint local (por defecto http://localhost:11434/api/generate)
# Otras variables opcionales:
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=llama3
OLLAMA_TEMPERATURE=0.2

# URL de la base de datos (Postgres) que Railway provisionará o que ya tengas
# Ejemplo: postgres://user:pass@host:5432/dbname
DATABASE_URL=

# Token interno para autenticación entre servicios (generado localmente o por el script)
# Puedes dejarlo vacío para que el script lo genere: PRAEVISIO_BEARER_TOKEN=auto
PRAEVISIO_BEARER_TOKEN=auto

# En el frontend Vite espera VITE_API_BASE_URL sin "/api" final
VITE_API_BASE_URL=

# Valor auxiliar que se usará en scripts si quieres sobreescribir
RAILWAY_BACKEND_URL=
